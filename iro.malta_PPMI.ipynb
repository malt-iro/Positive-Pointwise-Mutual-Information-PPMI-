{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Positive Pointwise Mutual Information</h2>\n",
    "<h2>Corpus Linguistics with Python</h1>\n",
    "<h3>Iro-Georgia Malta</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Corpus Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the lemmatization of the corpus, I import the lemmatizer of Standford CoreNLP. The code for the lemmatizer is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! java -cp \"/compLing/students/courses/corpusLinguistics/stanford-corenlp-4.4.0/*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,cleanxml,ssplit,pos,lemma -file \"/compLing/students/courses/corpusLinguistics/finalProject/text_fic.txt\" -outputFormat conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I use the output of the lemmatizer **text_fic.txt.conll** and I process it.\n",
    "2. I initialize an empty list **lemmatized_text**, where each lemmatized sentence will be stored.\n",
    "3. In a *for loop*, I use the following preprocessing steps:\n",
    "- I remove spaces at the beginning and at the end of each string with *strip()*. \n",
    "- I lowercase each string with *lower()*. \n",
    "- I split each string by the tabs with *split()*.\n",
    "\n",
    "4. The length of the sentence is determined by the length of the line. If the length of a line is equal to 7, only the **third element**, the lemma, is being appended to the new list **lemmatized_text**. If the length of the sentence is not equal to seven (empty line), it marks the end of the sentence. \n",
    "5. In the else statement, I create a new file **coca.preprocessed** with sentences (one sentence per line) containing only space separated lemmas.\n",
    "6. The list **lemmatized_text** is initialized again inside the else statement, so that the lemmas of the next sentence to be appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"text_fic.txt.conll\"\n",
    "new_corpus = \"coca.preprocessed\"\n",
    "\n",
    "with open(corpus_name, \"r\") as f:\n",
    "    lemmatized_text = []  # Initialize empty list\n",
    "    for line in f:\n",
    "        elements = line.strip().lower().split(\"\\t\")\n",
    "        if len(elements) == 7:  # [4, GATHERED, gather, VBD, _, _, _]\n",
    "            lemmatized_text.append(elements[2])  # Append the lemma (gather) to the list \n",
    "        else:\n",
    "            with open(new_corpus, \"a\") as fout:\n",
    "                print(\" \".join(lemmatized_text), end = '\\n', file = fout)\n",
    "            lemmatized_text = []  # Initialize empty list again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Counting Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word tokenization of the lemmatized corpus, I import the *word_tokenize* method from *nltk*. Additionally, I import the *bigrams* method from *nltk* to extract bigrams from the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, I load the lemmatized text file **coca.preprocessed** and store the lemmatized text to a new variable **text2**.\n",
    "2. Then, I tokenize it with the *word_tokenize* method and store it in variable **tokens**. \n",
    "3. After this point, I count all the unigrams of the text and I append a new key (lemma) and its value (count frequency) to the **unigramsDict**, when it is not present in the dictionary. If the lemma is present in the dictionary, its count frequency is increased by one. I set the frequency of unigrams to 0, if their frequency counts is lower or equal to 5.\n",
    "4. To extract bigrams from the lemmatized corpus, I use the *bigrams* method on the variable *tokens*. I convert the output of the *bigrams* method to a list, and I assign it to variable **bigrams_list**.\n",
    "5. I count all the bigrams from **bigrams_list** and I append a new key (lemma) and its value (count frequency) to the **bigramDict**, when it is not present in the dictionary. If the lemma is present in the dictionary, its count frequency is increased by one. I set the frequency of bigrams to 0, if their frequency counts is lower or equal to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name2 = \"coca.preprocessed\"\n",
    "new_corpus2 = \"coca.counts\"\n",
    "\n",
    "with open(corpus_name2, \"r\") as f:\n",
    "    text2 = f.read()\n",
    "    unigramsDict = {}  # Initialize unigramsDict\n",
    "    tokens = word_tokenize(text2)\n",
    "    for unigram in tokens:  # Count unigrams\n",
    "        if unigram not in unigramsDict:\n",
    "            unigramsDict[unigram] = 1\n",
    "        else:\n",
    "            unigramsDict[unigram] += 1\n",
    "    for unigram, freq in unigramsDict.items():\n",
    "        if freq <= 5:\n",
    "            unigramsDict[unigram] = 0       \n",
    "    bigrams_list = list(bigrams(tokens))\n",
    "    bigramDict = {}  # Initialize bigramDict\n",
    "    for bigram in bigrams_list:  # Count bigrams\n",
    "        if bigram not in bigramDict:\n",
    "            bigramDict[bigram] = 1\n",
    "        else:\n",
    "            bigramDict[bigram] += 1\n",
    "    for bigram, freq in bigramDict.items():\n",
    "        if freq <= 5:\n",
    "            bigramDict[bigram] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. I create a new text file **coca.counts**, in which I store the lemmas and their frequency counts from **unigramsDict** and **bigramDict** dictionaries as key-value pairs.\n",
    "7. I count the total number of tokens from **unigramsDict** by summing the frequency counts (values) of the unigrams (keys). The sum of the frequency counts is stored in variable **totalTokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_corpus2, \"a\") as fout:\n",
    "        totalTokens = 0  # Intialize variable totalTokens\n",
    "        for k, v in unigramsDict.items():\n",
    "            print(k, v, sep = '\\t', file = fout)\n",
    "            totalTokens += v  # Count total number of tokens from unigramsDict\n",
    "        print(totalTokens)  # Print total number of tokens\n",
    "        for k, v in bigramDict.items():\n",
    "            print(k, v, sep = '\\t', file = fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: PPMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the PPMI scores of the bigrams in the corpus, I import the library *math* and use its *log* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I define the **PPMI()** function, which has the arguments: **word1, word2, unigramsDict, bigramDict, totaltokens**.\n",
    "2. The **PPMI()** function returns:\n",
    "    - A score equal to 0.0:\n",
    "        - if **word1** (lemma) or **word2** are not inside **unigramsDict**, \n",
    "        - or the frequency counts of **word1** or **word2** (separately) is equal to 0,\n",
    "        - or **word1** and **word2** (occuring together) are not inside **bigramDict**,\n",
    "        - or their frequency counts (together) are equal to 0,\n",
    "        - or the PPMI score of a bigram is negative (since PPMI takes negative PMI values and turns them into zeros).\n",
    "    - The PPMI score:\n",
    "        - the *probabilties* of **word1** and **word2** (separately), divided by total frequency counts (totalTokens)\n",
    "        - the *joint probability* of **word1** and **word2**, divided by total frequency counts\n",
    "        - the final PPMI score is calculated with the *math.log()* method inside the *round()* function, and it is rounded to the third decimal position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(word1, word2, unigramsDict, bigramDict, Totaltokens):\n",
    "    if (word1 not in unigramsDict  # unigramsDict\n",
    "    or word2 not in unigramsDict \n",
    "    or unigramsDict[word1] == 0 \n",
    "    or unigramsDict[word2] == 0 \n",
    "    or (word1, word2) not in bigramDict  # bigramDict\n",
    "    or bigramDict[word1, word2] == 0):\n",
    "        PPMI_score = 0.0\n",
    "        return PPMI_score\n",
    "    else:\n",
    "        word1_prob = unigramsDict[word1] / Totaltokens  # P(x)\n",
    "        word2_prob = unigramsDict[word2] / Totaltokens  # P(y)\n",
    "        prob_word1_word2 = bigramDict[word1, word2] / Totaltokens  # P(x,y)\n",
    "        PPMI_score = round(math.log(prob_word1_word2 \n",
    "                                    / (word1_prob*word2_prob), 2), 3)\n",
    "        if PPMI_score < 0:  # If any negative scores\n",
    "            PPMI_score = 0.0\n",
    "            return PPMI_score\n",
    "        else:\n",
    "            return PPMI_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I call the function to check if it works properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI(',', 'have', unigramsDict, bigramDict, totalTokens)  # negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI('croatia', 'crete', unigramsDict, bigramDict, totalTokens)  # new lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI('who', 'leave', unigramsDict, bigramDict, totalTokens)  # existing lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Computing PPMI Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I choose **dictionary** as the data structure, where I store all the bigrams and their PPMI scores. \n",
    "2. I use *for loop* to go through the bigrams (keys) of **bigramDict**. Inside the loop I use the *PPMI()* function, which contains: **bigram[0], bigram[1], unigramsDict, bigramDict** and **totalTokens**. To access the items of the bigramDict, I use *the indices [0] and [1]* because the bigrams are stored as tuples in the dictionary. \n",
    "3. Once PPMI score is computed, each bigram (key) and its PPMI score (value) is stored to the new dictionary **PPMI_bigrams_score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI_bigrams_score = {}  # Initialize empty dictionary\n",
    "\n",
    "for bigram in bigramDict:\n",
    "    score = PPMI(bigram[0], bigram[1], unigramsDict, bigramDict, totalTokens)\n",
    "    PPMI_bigrams_score[bigram] = score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, I create a new text file **coca.ppmi**, where I print each bigram (key) and their PPMI score (value) from the dictionary **PPMI_bigrams_score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file4 = \"coca.ppmi\"\n",
    "\n",
    "with open(new_file4, \"a\") as fout:\n",
    "    for bigram, score in PPMI_bigrams_score.items():\n",
    "        print(bigram[0], bigram[1], score, sep = '\\t', file = fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I define the new function **topN()**. The function expects a dictionary (**PPMI_bigramScores**) and has the agrument **n=20**, which indicates the default value of the number of bigrams to visualise. The contents of the function are:\n",
    "1. I sort the PPMI scores in a descending order using the built-in function **sorted()** and its reverse argument, and I store it to variable **bigrams_sorted**.\n",
    "2. Since the list is sorted in a descending order, the first 20 elements of the list are the top 20 scores.\n",
    "3. To store the top 20 bigrams, which have the same PPMI score from sorted list **bigrams_sorted**: I look up the corresponding value from the **PPMI_bigramScores** dictionary and I match the key (bigrams) to its corresponding score from **bigrams_sorted**. Then, I add the key-value pairs to the new dictionary **sorted_dict**.\n",
    "4. Lastly, the function prints the first **[0]** and the second **[1]** lemma of each bigram as well as their PPMI scores in the cell, *tab separated* from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(PPMI_bigramScores, n = 20):\n",
    "    bigrams_sorted = sorted(PPMI_bigramScores.values(), reverse=True)\n",
    "    sorted_dict = {}  # Initialize empty dictionary\n",
    "    for score in bigrams_sorted[:n]:  # Loop through list\n",
    "        for bigram in PPMI_bigramScores.keys():  # Loop through dictionary\n",
    "            if PPMI_bigramScores[bigram] == score:\n",
    "                sorted_dict[bigram] = PPMI_bigramScores[bigram]\n",
    "    for k, v in sorted_dict.items():\n",
    "        print(k[0], k[1], v, sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I call the **topN()** function and I pass to it the **PPMI_bigrams_score** dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN(PPMI_bigrams_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Comparing PPMI and Frequency Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing differences between PPMI scores and frequency counts:\n",
    "\n",
    "**PPMI scores**:\n",
    "- PPMI scores show the possibility of two words (bigram) occuring together in the corpus.\n",
    "- The calculation of PPMI scores involves many computations.\n",
    "- The bigrams with the highest PPMI scores are **named entities**.\n",
    "- **Named entities** are useful information.\n",
    "\n",
    "**Frequency Counts**:\n",
    "- Frequency counts show how many times a bigram occurs in the corpus.\n",
    "- The calculation of frequency counts involves only the summing of the number of occurrences from each word of a bigram.\n",
    "- The bigrams with the highest frequency counts are mainly **punctuation** and **function words** (e.g., conjunctions, pronouns, articles, prepositions and auxiliary verbs). **Function words** do not provide any useful information about the context of a text.\n",
    "- However, the bigram *'do'* and *'not'* contains the **content word** *'not'*, which belongs to **negation**. **Content words** such as **negation** provide important information about the context of a text.\n",
    "\n",
    "**<u>Conclusion</u>**:\n",
    "The bigrams with highest PPMI scores are **named entities**, while the bigrams with the highest frequency counts are mainly **punctuation** and **function words**. **Named entities** are considered useful information, while **punctuation** and **function words** are not because they do not provide important information about the context of a text. The frequency counts of **punctuation** and **function words** are the highest ones because they occur more frequently in a corpus than content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN(bigramDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
